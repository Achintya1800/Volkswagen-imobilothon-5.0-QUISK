# ğŸš— Volkswagen Intelligent Hazard Detection System

### ğŸ“˜ Overview
AI-driven hazard detection and feedback learning system for smart vehicles.  
Integrates **computer vision**, **sensor fusion**, and **edgeâ€“cloud intelligence** to detect and alert drivers about potholes, bumps, debris, or stalled vehicles.

---

## ğŸ§  1ï¸âƒ£ Architecture Overview
![System Architecture](./images/architecture_diagram.png)

ğŸ¥ Demo Video:  
<video width="640" controls>
  <source src="./videos/Volkswagen_Quisk.mp4" type="video/mp4">
</video>

---

## ğŸ—ºï¸ 2ï¸âƒ£ Offline Map Preparation

ğŸ¯ **Objective:**  
Enable safe and optimized navigation by combining **real-time** and **offline hazard-aware routing**.

âš™ï¸ **Key Highlights:**  
- **Startâ€“End Input:** Driver selects source and destination (works online & offline).  
- **Smart Route Fetching:** Fetches the safest & most efficient route using preloaded hazard data.  
- **Hazard Overlay:** Displays potholes, bumps, and debris along the route using YOLO-based detection.  
- **Voice Alerts:** Real-time voice guidance warns the driver of upcoming hazards.  
- **Offline Mode:** Operates seamlessly with pre-fetched routes and hazard cache.  
- **Hazard Density Slider:** Allows users to simulate or adjust hazard visibility on the map.  

![Offline Map Preparation](./images/Driver.png)

---

## ğŸš— 3ï¸âƒ£ Image Capture Rate Controller (AKS)

ğŸ¯ **Objective:**  
Optimize video processing by capturing only key frames through **Adaptive Keyframe Sampling (AKS)**.

ğŸ” **Overview:**  
- Monitors video stream, motion, and scene content in real time.  
- Skips redundant frames when the car is static or moving slowly.  
- Captures more frames during rapid motion or new object appearance.  
- Processes **only keyframes** to maintain efficiency without losing critical details.  

âš™ï¸ **Impact:**  
Reduces compute load and latency by **up to 60%**, while preserving essential scene information.

ğŸ“Š **Performance Summary**

| Scenario | Avg Speed | Keyframes Saved | Behavior |
|-----------|------------|------------------|-----------|
| ğŸš¦ City / Traffic | 0.18 km/h | 20.08% | Slow â†’ Frames skipped |
| ğŸ›£ï¸ Highway | 13.07 km/h | 23.82% | Fast â†’ More frames saved |

ğŸ¥ **Demo:**  
<iframe src="https://drive.google.com/file/d/1XoV6hV5G-eiYsWmdHrJb4z8seiDvawzX/preview" 
        width="720" height="480" allow="autoplay">
</iframe>

---

## ğŸ§© 4ï¸âƒ£ Composite Image Generation

Combines multiple camera feeds into a **single unified frame** using OpenCV.

ğŸ§  **Pipeline**
1. **Multiple Image Frames** â†’ Input from different cameras  
2. **Fisheye Distortion Correction** â†’ Removes lens curvature and fixes 180Â°+ distortion  
3. **Inverse Perspective Mapping (IPM)** â†’ Aligns ground plane and maintains consistent scale  
4. **Multi-Band Blending (Laplacian Pyramids)** â†’ Seamless merging, removes seams  
5. **Unified Frame** â†’ Final panoramic composite  

![Input](./images/composite.png)

---

## ğŸ§© 5ï¸âƒ£ Image Quality Enhancement (DehazeNet)

Enhances **foggy, hazy, or low-light** frames using a **lightweight GAN-based AOD-Net** to restore clarity and boost detection accuracy.

**Key Points:**
- 5-layer CNN with skip connections  
- Fast, CPU-friendly, deployable on mobile  
- Formula: `clean = ReLU((x5 * x) - x5 + 1)`  
- Input â†’ RGB image | Output â†’ Dehazed RGB image  

ğŸ¯ **Goal:** Restore clear visuals for reliable vision-based detection.

ğŸ–¼ï¸ Before vs After Dehazing:  
![Dehaze Example](./images/dehaze.png)

---

## ğŸ§© 6ï¸âƒ£ Feature Extractor
The **Feature Extractor** is the vision intelligence core of the system.  
It performs **multi-head visual analysis** to detect, segment, measure, and track road hazards with ego-motion compensation.

| Head | Task | Model |
|------|------|--------|
| 1 | Object Detection (Pothole, Bump, Debris) | YOLOv10 + DINOv3 Distillation |
| 2 | Segmentation (Pothole Size Estimation) | YOLOv8 (self-trained) |
| 3 | Depth & Severity Estimation | MiDaS-small |
| 4 | Road Mask (On-road or Pavement) | Fast-SCNN |
| 5 | Stalled Vehicle Tracking | YOLOv10 + SORT (Kalman Filter) + Affine RANSAC |

---

### ğŸ§© Head 1: Detection
- Detects **potholes, bumps, and debris** using **YOLOv10**.
- Enhanced via **DINOv3 â†’ YOLOv10 distillation**, transferring texture-rich features for better accuracy.
- Provides base bounding boxes for later modules.

![Head 1 â€“ Detection](./images/H1.png)

---

### ğŸ§© Head 2: Segmentation

ğŸ¯ **Goal**  
Detect and segment **pothole regions** using a **YOLOv8-Segmentation** model to obtain **pixel-level masks** that quantify the damaged road surface area.

**Model Summary**

| Parameter | Description |
|------------|--------------|
| **Input** | 640Ã—640 RGB road image |
| **Output** | Bounding box + Mask for each detected pothole |
| **Model File** | `YoloV8Segmented.pt` |

ğŸ§® **Formula**  
`Pothole Area = Î£(mask_pixels) Ã— (pixel_scale)^2`

ğŸ“Š **Performance Metrics**

| Metric | Value |
|---------|--------|
| Precision | 0.76 |
| Recall | 0.59 |
| mAP50 | 0.72 |
| mAP50â€“95 | 0.45 |
| Fitness | 0.878 |

![Head 2 â€“ Segmentation](./images/segmentation.png)

---

### ğŸ§© Head 3: Depth & Severity Estimation

**Overview**  
Estimates the **depth**, **width**, and **area** of detected potholes using **MiDaS v2.1 Small**.  
Each pothole is then classified as **Minor**, **Moderate**, or **Severe** based on **depthâ€“area thresholds**.

**Workflow**
1. Take **segmented pothole masks** from YOLOv8-Segmentation output.  
2. Generate a **relative depth map** using **MiDaS-small (256)**.  
3. Compute:
   - **Area** â†’ from mask pixels  
   - **Width / Height** â†’ from bounding box  
   - **Depth** â†’ mean depth in masked region  
4. Combine **depth and area** to classify severity.  

ğŸ§® **Formula for Apparent Size**
`S = f * X / Z`

**Where:**
- f = focal length  
- X = observed pixel width  
- Z = distance from camera  

ğŸ–¼ï¸ **Output Example**  
![Head 3 â€“ Depth Estimation](./images/midas.png)

---

### ğŸ§© Head 4: Road Mask

ğŸ¯ **Goal**  
Determine whether each detected object lies **on the drivable surface** or **off-road/pavement**.  
Helps filter false hazards and improves safety analytics.

âš™ï¸ **Architecture Overview**

| Head | Purpose | Model Used | Input | Output |
|------|----------|-------------|--------|---------|
| 1 | Road Segmentation | ğŸ§© Fast-SCNN (Cityscapes-trained) | RGB image | Binary road mask |
| 2 | Pothole Masking | ğŸ¯ YOLOv8-Seg (custom-trained) | RGB image | Pothole masks |
| 3 | Vehicle Segmentation | ğŸš— YOLOv11n-Seg (COCO-pretrained) | RGB image | Vehicle masks |

ğŸ§® **Computation**
`on_road_ratio = overlap_area / object_area`

ğŸ–¼ï¸ **Example Output**  
![Head 4 â€“ Road Mask](./images/mask.png)

---

### ğŸ§© Head 5: Stalled Vehicles Tracking
- Detects and tracks **stalled vehicles** using **YOLOv10 + SORT (Kalman Filter)**.  
- Compensates for **camera motion** using **Affine RANSAC** on background features.

ğŸ§® **Ego-Motion Compensation Formula:**
![Head 5 â€“ Formula](./images/stallformula.png)

ğŸ¥ **Demo:**  
<iframe src="https://drive.google.com/file/d/1Lp4UQRNKpWlADlHd_ZKrdra-lUGOxaDZ/preview" 
        width="720" height="480" allow="autoplay">
</iframe>

---

## âœ… 7ï¸âƒ£ Fusion Engine

ğŸ¯ **Goal**  
Fuse outputs from all perception heads into one reliable **confidence score (C_final)** for final hazard decision.

âš™ï¸ **Workflow**
1. Collect outputs from all heads.  
2. Compute individual confidence scores.  
3. Apply **fusion rules** â€” Weighted, Multiplicative, or Rule-Based.  
4. Generate final **C_final** for verification and alert modules.

ğŸ§® **Individual Confidence Scores**

| Head | Metric | Formula |
|------|---------|----------|
| Detection | Class Ã— IoU | `C_detect = P_class Ã— IoU` |
| Segmentation | Mask ratio | `C_seg = Area_mask / Area_bbox` |
| Depth | Error decay | `exp(-depth_error / Ïƒ)` |
| Road | On-road penalty | `C_road = {1.0 if on_road, Î± if off_road}` |
| Tracking | Persistence | `C_track = min(track_age / T_stable, 1) Ã— (1 - miss_rate)` |

---

## ğŸ”’ 8ï¸âƒ£ Privacy Blur

ğŸ¯ **Goal**  
Blur faces and license plates **on-device** before upload using **YOLOv8 + Gaussian Blur**.

âš™ï¸ **Workflow**
1. Frame input from Fusion Engine.  
2. Detect faces & plates using YOLOv8.  
3. Apply Gaussian blur using OpenCV.  
4. Forward anonymized output downstream.

ğŸ–¼ï¸ **Results**

| Face Blur | Number Plate Blur |
|------------|------------------|
| ![Face Blur](./images/blurred_privacy1.png) | ![Plate Blur](./images/blurred_privacy2.png) |

---

## ğŸ—£ï¸ 9ï¸âƒ£ Voice-Based Verification

ğŸ¯ **Goal**  
Validate detected hazards through **trusted driver voice feedback** using a **GenAI verification interface**.

âš™ï¸ **Workflow**
1. Send hazard clip to trusted driver.  
2. Collect **Yes / No / Unsure** response.  
3. Apply **Speech-to-Text** + Bayesian trust update.  
4. Confirmed hazards are uploaded; rejected ones discarded.

ğŸ§® **Confidence Adjustment**
`C_verified = C_final Ã— Î²`  
(`Î² = 1.2` if verified, `0.5` if unverified)

ğŸ§® **Trust Update**
`Trust_new = Trust_prev + Î· Ã— (Verification_result - Expected)`

ğŸ¥ **Demo:**  
<iframe src="https://drive.google.com/file/d/1vjpDZbSEpLwrKn-KVpwRr5L3FaaPR5wL/preview" 
        width="720" height="480" allow="autoplay">
</iframe>

---

## ğŸ§  ğŸ”Ÿ Feedback Learning (Federated PPO)

ğŸ¯ **Goal / Motto**  
â€œEvery vehicle learns â€” the fleet gets smarter.â€  
Uses **federated PPO learning** to improve detection without sharing raw data.

âš™ï¸ **Process**
1. Vehicles train locally.  
2. Send model updates (not data) to server.  
3. Server aggregates via PPO.  
4. Global model redistributed â†’ continuous improvement.

---

## âš¡ 1ï¸âƒ£1ï¸âƒ£ Alert System

ğŸ¯ **Goal**
Ensure reliable, **non-redundant hazard uploads** from drivers to the server â€” even offline.

âš™ï¸ **Workflow**
1. **Connectivity Check**
   - âœ… Internet â†’ Send via **MQTT**  
   - âŒ Offline â†’ Store in local cache
2. **Server Receives Data**
   - MQTT topic `/hazard/detections`
3. **Clustering (Haversine Formula)**
   - Merge nearby hazards to prevent duplicates.

ğŸ§® **Formula**
`d = 2R Ã— arcsin(âˆša)`  
`a = sinÂ²(Î”lat/2) + cos(latâ‚) Ã— cos(latâ‚‚) Ã— sinÂ²(Î”lon/2)`


---

## ğŸ§© 1ï¸âƒ£2ï¸âƒ£ Tech Stack

### ğŸ¤– **AI / ML Frameworks**
- **YOLOv10 / YOLOv11** â†’ Object detection for potholes, debris, stalled vehicles  
- **YOLOv8-Seg** â†’ Pixel-level segmentation for pothole shape & area estimation  
- **MiDaS-small v2.1** â†’ Depth & severity estimation  
- **Fast-SCNN** â†’ Lightweight road mask segmentation (drivable area classification)  
- **DINOv3 Distillation** â†’ Feature-rich representation transfer to YOLOv10  
- **PPO (Proximal Policy Optimization)** â†’ Federated feedback learning algorithm  
- **AOD-Net / DehazeNet** â†’ Low-light & haze restoration for image clarity  
- **SORT / StrongSORT** â†’ Object tracking with Kalman Filter + IoU Matching  




---

### â˜ï¸ **Cloud & Communication**
- **MQTT / HTTPS** â†’ Data transfer between vehicle â†’ cloud â†’ app  
- **V2X Communication (DSRC / MQTT Topics)** â†’ Peer-to-peer vehicle alerts  
- **Haversine Clustering** â†’ In-server deduplication of hazard reports  

---

### ğŸ”’ **Security & Privacy**
- **YOLOv8 Face + Plate Detector** â†’ Privacy blur before upload  
- **SSL / TLS** â†’ Secure data transmission  
- **Tokenized Session Handling** â†’ Avoids redundant hazard submissions  

---

### ğŸ§­ **Mapping & Navigation**
- **OSM (OpenStreetMap) APIs** â†’ Map rendering & hazard overlay  
- **Leaflet.js / Mapbox** â†’ Real-time road visualization  
- **Offline Tile Caching** â†’ Hazard-aware route navigation when offline  






---

ğŸ§  **Summary:**  
The pipeline spans **edge AI, federated learning, and cloud analytics**, ensuring reliability, privacy, and real-time hazard awareness from detection â†’ verification â†’ alert dissemination.


---

## ğŸ“‚ 1ï¸âƒ£3ï¸âƒ£ Project Structure


---

##  Impact
> Enhances road safety by detecting and verifying real-world hazards in real time â€” even in fog, low-light, or offline conditions.
